{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy vs Beta Analysis\n",
    "\n",
    "This notebook analyzes how conditional entropy (the theoretical minimum loss) varies with beta for different vocabulary sizes.\n",
    "\n",
    "- **Y-axis**: Conditional Entropy H(X_{t+1} | X_t)\n",
    "- **X-axis**: Beta (temperature parameter)\n",
    "- **Comparison**: Different vocabulary sizes\n",
    "- **Methods**: Both analytical and empirical estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import RLM_files.datasets as datasets\n",
    "import RLM_files.measures as measures\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "vocab_sizes = [32, 64, 128, 256]  # Different vocabulary sizes to compare\n",
    "beta_values = np.linspace(0.5, 5.0, 20)  # Range of beta values\n",
    "L = 1  # Tree depth (2^L leaves)\n",
    "\n",
    "# Seeds for reproducibility\n",
    "seed_rules = 12345678\n",
    "seed_samples = 56781234\n",
    "\n",
    "# Sample sizes for estimation\n",
    "num_analytical_samples = 2**15  # For analytical estimation\n",
    "num_empirical_samples = 50000   # For empirical estimation\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Entropy Estimation\n",
    "\n",
    "Uses the analytical method from `measures.conditional_entropy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analytical_entropy(vocab_size, beta, L, seed_rules, num_samples):\n",
    "    \"\"\"\n",
    "    Compute conditional entropy using analytical method.\n",
    "    \"\"\"\n",
    "    # Create RLM\n",
    "    rlm = datasets.RLM(\n",
    "        v=vocab_size,\n",
    "        L=L,\n",
    "        beta=beta,\n",
    "        seed_rules=seed_rules,\n",
    "        seed_samples=seed_samples,\n",
    "        num_data=None,\n",
    "        probs=None,\n",
    "        transform=None,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Compute analytical entropy\n",
    "    conditional_entropy = measures.conditional_entropy(rlm.M, vocab_size, num_samples)\n",
    "    marginal_entropy = measures.marginal(rlm.M, vocab_size, num_samples)\n",
    "    \n",
    "    return conditional_entropy, marginal_entropy\n",
    "\n",
    "\n",
    "# Compute analytical entropies\n",
    "print(\"Computing analytical entropies...\")\n",
    "analytical_results = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "    conditional_entropies = []\n",
    "    marginal_entropies = []\n",
    "    \n",
    "    for beta in tqdm(beta_values, desc=f\"V={vocab_size}\"):\n",
    "        cond_ent, marg_ent = compute_analytical_entropy(\n",
    "            vocab_size, beta, L, seed_rules, num_analytical_samples\n",
    "        )\n",
    "        conditional_entropies.append(cond_ent)\n",
    "        marginal_entropies.append(marg_ent)\n",
    "    \n",
    "    analytical_results[vocab_size] = {\n",
    "        'conditional': np.array(conditional_entropies),\n",
    "        'marginal': np.array(marginal_entropies)\n",
    "    }\n",
    "\n",
    "print(\"\\nAnalytical computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Entropy Estimation\n",
    "\n",
    "Uses empirical bigram distribution from generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_entropy(vocab_size, beta, L, seed_rules, num_samples):\n",
    "    \"\"\"\n",
    "    Compute conditional entropy using empirical method.\n",
    "    \"\"\"\n",
    "    # Create RLM\n",
    "    rlm = datasets.RLM(\n",
    "        v=vocab_size,\n",
    "        L=L,\n",
    "        beta=beta,\n",
    "        seed_rules=seed_rules,\n",
    "        seed_samples=seed_samples,\n",
    "        num_data=None,\n",
    "        probs=None,\n",
    "        transform=None,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Compute empirical statistics\n",
    "    stats = rlm.compute_all_empirical_statistics(num_samples=num_samples)\n",
    "    \n",
    "    return stats['conditional_entropy'], stats['marginal_entropy']\n",
    "\n",
    "\n",
    "# Compute empirical entropies\n",
    "print(\"Computing empirical entropies...\")\n",
    "empirical_results = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "    conditional_entropies = []\n",
    "    marginal_entropies = []\n",
    "    \n",
    "    for beta in tqdm(beta_values, desc=f\"V={vocab_size}\"):\n",
    "        cond_ent, marg_ent = compute_empirical_entropy(\n",
    "            vocab_size, beta, L, seed_rules, num_empirical_samples\n",
    "        )\n",
    "        conditional_entropies.append(cond_ent)\n",
    "        marginal_entropies.append(marg_ent)\n",
    "    \n",
    "    empirical_results[vocab_size] = {\n",
    "        'conditional': np.array(conditional_entropies),\n",
    "        'marginal': np.array(marginal_entropies)\n",
    "    }\n",
    "\n",
    "print(\"\\nEmpirical computation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Conditional Entropy vs Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot analytical results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Analytical\n",
    "for vocab_size in vocab_sizes:\n",
    "    ax1.plot(\n",
    "        beta_values,\n",
    "        analytical_results[vocab_size]['conditional'],\n",
    "        marker='o',\n",
    "        label=f'V={vocab_size}',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel('Beta (Temperature)', fontsize=14)\n",
    "ax1.set_ylabel('Conditional Entropy H(X_{t+1} | X_t)', fontsize=14)\n",
    "ax1.set_title('Analytical Conditional Entropy vs Beta', fontsize=16, fontweight='bold')\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Empirical\n",
    "for vocab_size in vocab_sizes:\n",
    "    ax2.plot(\n",
    "        beta_values,\n",
    "        empirical_results[vocab_size]['conditional'],\n",
    "        marker='s',\n",
    "        label=f'V={vocab_size}',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel('Beta (Temperature)', fontsize=14)\n",
    "ax2.set_ylabel('Conditional Entropy H(X_{t+1} | X_t)', fontsize=14)\n",
    "ax2.set_title('Empirical Conditional Entropy vs Beta', fontsize=16, fontweight='bold')\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Analytical vs Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare analytical and empirical for each vocabulary size\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, vocab_size in enumerate(vocab_sizes):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot both methods\n",
    "    ax.plot(\n",
    "        beta_values,\n",
    "        analytical_results[vocab_size]['conditional'],\n",
    "        marker='o',\n",
    "        label='Analytical',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "    ax.plot(\n",
    "        beta_values,\n",
    "        empirical_results[vocab_size]['conditional'],\n",
    "        marker='s',\n",
    "        label='Empirical',\n",
    "        linewidth=2,\n",
    "        markersize=6,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Beta (Temperature)', fontsize=12)\n",
    "    ax.set_ylabel('Conditional Entropy', fontsize=12)\n",
    "    ax.set_title(f'Vocabulary Size = {vocab_size}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Entropy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot marginal entropy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Analytical\n",
    "for vocab_size in vocab_sizes:\n",
    "    ax1.plot(\n",
    "        beta_values,\n",
    "        analytical_results[vocab_size]['marginal'],\n",
    "        marker='o',\n",
    "        label=f'V={vocab_size}',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "    # Add theoretical maximum (log V)\n",
    "    ax1.axhline(\n",
    "        y=np.log(vocab_size),\n",
    "        linestyle='--',\n",
    "        alpha=0.5,\n",
    "        label=f'log({vocab_size})' if vocab_size == vocab_sizes[0] else None\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel('Beta (Temperature)', fontsize=14)\n",
    "ax1.set_ylabel('Marginal Entropy H(X_{t+1})', fontsize=14)\n",
    "ax1.set_title('Analytical Marginal Entropy vs Beta', fontsize=16, fontweight='bold')\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Empirical\n",
    "for vocab_size in vocab_sizes:\n",
    "    ax2.plot(\n",
    "        beta_values,\n",
    "        empirical_results[vocab_size]['marginal'],\n",
    "        marker='s',\n",
    "        label=f'V={vocab_size}',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "    # Add theoretical maximum (log V)\n",
    "    ax2.axhline(\n",
    "        y=np.log(vocab_size),\n",
    "        linestyle='--',\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel('Beta (Temperature)', fontsize=14)\n",
    "ax2.set_ylabel('Marginal Entropy H(X_{t+1})', fontsize=14)\n",
    "ax2.set_title('Empirical Marginal Entropy vs Beta', fontsize=16, fontweight='bold')\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Conditional Entropy Range by Vocabulary Size\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\nVocabulary Size: {vocab_size}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Analytical\n",
    "    anal_cond = analytical_results[vocab_size]['conditional']\n",
    "    print(f\"Analytical Conditional Entropy:\")\n",
    "    print(f\"  Min (β={beta_values[np.argmin(anal_cond)]:.2f}): {anal_cond.min():.4f}\")\n",
    "    print(f\"  Max (β={beta_values[np.argmax(anal_cond)]:.2f}): {anal_cond.max():.4f}\")\n",
    "    print(f\"  Range: {anal_cond.max() - anal_cond.min():.4f}\")\n",
    "    \n",
    "    # Empirical\n",
    "    emp_cond = empirical_results[vocab_size]['conditional']\n",
    "    print(f\"\\nEmpirical Conditional Entropy:\")\n",
    "    print(f\"  Min (β={beta_values[np.argmin(emp_cond)]:.2f}): {emp_cond.min():.4f}\")\n",
    "    print(f\"  Max (β={beta_values[np.argmax(emp_cond)]:.2f}): {emp_cond.max():.4f}\")\n",
    "    print(f\"  Range: {emp_cond.max() - emp_cond.min():.4f}\")\n",
    "    \n",
    "    # Difference\n",
    "    diff = np.abs(anal_cond - emp_cond)\n",
    "    print(f\"\\nAnalytical vs Empirical:\")\n",
    "    print(f\"  Mean absolute difference: {diff.mean():.4f}\")\n",
    "    print(f\"  Max absolute difference: {diff.max():.4f}\")\n",
    "    \n",
    "    # Theoretical maximum\n",
    "    print(f\"\\nTheoretical Maximum (log V): {np.log(vocab_size):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later analysis\n",
    "results = {\n",
    "    'beta_values': beta_values,\n",
    "    'vocab_sizes': vocab_sizes,\n",
    "    'analytical': analytical_results,\n",
    "    'empirical': empirical_results,\n",
    "    'config': {\n",
    "        'L': L,\n",
    "        'seed_rules': seed_rules,\n",
    "        'seed_samples': seed_samples,\n",
    "        'num_analytical_samples': num_analytical_samples,\n",
    "        'num_empirical_samples': num_empirical_samples\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(results, 'entropy_vs_beta_results.pt')\n",
    "print(\"\\nResults saved to 'entropy_vs_beta_results.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
