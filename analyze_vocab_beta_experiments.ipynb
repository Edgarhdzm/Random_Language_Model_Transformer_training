{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary & Beta Experiments Analysis\n",
    "\n",
    "This notebook analyzes results across multiple vocabulary sizes and beta values:\n",
    "- Beta values: [0.6, 0.8, 1.2, 1.8, 2.4, 3.0]\n",
    "- Vocabulary sizes: [64, 128, 256]\n",
    "- 6 repetitions per configuration\n",
    "\n",
    "**Analysis:**\n",
    "1. Loss curves for each vocab size\n",
    "2. Comparison across beta values\n",
    "3. Comparison across vocabulary sizes\n",
    "4. Check convergence to theoretical minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "beta_values = [0.6, 0.8, 1.2, 1.8, 2.4, 3.0]\n",
    "vocab_sizes = [64, 128, 256]\n",
    "num_repetitions = 6\n",
    "num_layers = 1\n",
    "\n",
    "# Results directory\n",
    "results_dir = Path('results/vocab_beta_experiments')\n",
    "\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Beta values: {beta_values}\")\n",
    "print(f\"Vocabulary sizes: {vocab_sizes}\")\n",
    "print(f\"Repetitions: {num_repetitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(vocab_size, beta, rep):\n",
    "    \"\"\"Load results for a specific configuration.\"\"\"\n",
    "    filename = results_dir / f'rlm_v{vocab_size}_L{num_layers}_beta{beta}_rep{rep}.pt'\n",
    "    \n",
    "    if not filename.exists():\n",
    "        print(f\"Warning: File not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    data = torch.load(filename, weights_only=False)\n",
    "    return data\n",
    "\n",
    "# Load all results\n",
    "all_results = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    all_results[vocab_size] = {}\n",
    "    \n",
    "    for beta in beta_values:\n",
    "        all_results[vocab_size][beta] = []\n",
    "        \n",
    "        for rep in range(1, num_repetitions + 1):\n",
    "            result = load_experiment_results(vocab_size, beta, rep)\n",
    "            \n",
    "            if result is not None:\n",
    "                all_results[vocab_size][beta].append(result)\n",
    "        \n",
    "        print(f\"V={vocab_size}, β={beta}: Loaded {len(all_results[vocab_size][beta])} results\")\n",
    "\n",
    "print(\"\\nAll results loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Theoretical Minimums and Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamics(result_data):\n",
    "    \"\"\"Extract training dynamics from result data.\"\"\"\n",
    "    if 'output' not in result_data:\n",
    "        return None\n",
    "    \n",
    "    dynamics = result_data['output']['dynamics']\n",
    "    \n",
    "    steps = [d['t'] for d in dynamics]\n",
    "    test_losses = [d['testloss'] for d in dynamics]\n",
    "    test_accs = [d['testacc'] for d in dynamics]\n",
    "    \n",
    "    return {\n",
    "        'steps': np.array(steps),\n",
    "        'test_loss': np.array(test_losses),\n",
    "        'test_acc': np.array(test_accs)\n",
    "    }\n",
    "\n",
    "# Extract theoretical minimums and dynamics\n",
    "theoretical_minimums = {}\n",
    "marginal_entropies = {}\n",
    "all_dynamics = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    theoretical_minimums[vocab_size] = {}\n",
    "    marginal_entropies[vocab_size] = {}\n",
    "    all_dynamics[vocab_size] = {}\n",
    "    \n",
    "    for beta in beta_values:\n",
    "        theoretical_minimums[vocab_size][beta] = []\n",
    "        marginal_entropies[vocab_size][beta] = []\n",
    "        all_dynamics[vocab_size][beta] = []\n",
    "        \n",
    "        for result in all_results[vocab_size][beta]:\n",
    "            if result is not None and 'output' in result:\n",
    "                # Extract pre-computed entropy\n",
    "                entropy = result['output']['entropy']\n",
    "                marginal = result['output']['marginal']\n",
    "                theoretical_minimums[vocab_size][beta].append(entropy)\n",
    "                marginal_entropies[vocab_size][beta].append(marginal)\n",
    "                \n",
    "                # Extract dynamics\n",
    "                dynamics = extract_dynamics(result)\n",
    "                if dynamics is not None:\n",
    "                    all_dynamics[vocab_size][beta].append(dynamics)\n",
    "\n",
    "print(\"Theoretical minimums and dynamics extracted!\")\n",
    "print(\"\\nSummary:\")\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "    for beta in beta_values:\n",
    "        if len(theoretical_minimums[vocab_size][beta]) > 0:\n",
    "            mean_ent = np.mean(theoretical_minimums[vocab_size][beta])\n",
    "            print(f\"  β={beta}: Conditional entropy = {mean_ent:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Loss Curves for Each Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_size in vocab_sizes:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(beta_values)))\n",
    "    \n",
    "    for idx, beta in enumerate(beta_values):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if len(all_dynamics[vocab_size][beta]) == 0:\n",
    "            ax.text(0.5, 0.5, f'No data for β={beta}', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "        \n",
    "        # Get common steps\n",
    "        steps = all_dynamics[vocab_size][beta][0]['steps']\n",
    "        \n",
    "        # Collect all losses\n",
    "        all_losses = []\n",
    "        for dynamics in all_dynamics[vocab_size][beta]:\n",
    "            if len(dynamics['steps']) == len(steps):\n",
    "                all_losses.append(dynamics['test_loss'])\n",
    "        \n",
    "        if len(all_losses) == 0:\n",
    "            continue\n",
    "        \n",
    "        all_losses = np.array(all_losses)\n",
    "        mean_loss = np.mean(all_losses, axis=0)\n",
    "        std_loss = np.std(all_losses, axis=0)\n",
    "        \n",
    "        # Plot mean with shaded std\n",
    "        ax.plot(steps, mean_loss, linewidth=2, label='Mean test loss', color=colors[idx])\n",
    "        ax.fill_between(steps, mean_loss - std_loss, mean_loss + std_loss, \n",
    "                         alpha=0.3, color=colors[idx], label='± 1 std')\n",
    "        \n",
    "        # Plot theoretical minimum\n",
    "        if len(theoretical_minimums[vocab_size][beta]) > 0:\n",
    "            mean_theoretical = np.mean(theoretical_minimums[vocab_size][beta])\n",
    "            ax.axhline(y=mean_theoretical, color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Theoretical: {mean_theoretical:.3f}')\n",
    "        \n",
    "        ax.set_xlabel('Training Steps', fontsize=11)\n",
    "        ax.set_ylabel('Test Loss', fontsize=11)\n",
    "        ax.set_title(f'β = {beta}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "    \n",
    "    fig.suptitle(f'Vocabulary Size = {vocab_size}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Compare All Betas for Each Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_size in vocab_sizes:\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(beta_values)))\n",
    "    \n",
    "    for idx, beta in enumerate(beta_values):\n",
    "        if len(all_dynamics[vocab_size][beta]) == 0:\n",
    "            continue\n",
    "        \n",
    "        steps = all_dynamics[vocab_size][beta][0]['steps']\n",
    "        \n",
    "        # Collect all losses\n",
    "        all_losses = []\n",
    "        for dynamics in all_dynamics[vocab_size][beta]:\n",
    "            if len(dynamics['steps']) == len(steps):\n",
    "                all_losses.append(dynamics['test_loss'])\n",
    "        \n",
    "        if len(all_losses) == 0:\n",
    "            continue\n",
    "        \n",
    "        all_losses = np.array(all_losses)\n",
    "        mean_loss = np.mean(all_losses, axis=0)\n",
    "        std_loss = np.std(all_losses, axis=0)\n",
    "        \n",
    "        # Plot mean\n",
    "        ax.plot(steps, mean_loss, linewidth=2.5, label=f'β={beta}', color=colors[idx])\n",
    "        ax.fill_between(steps, mean_loss - std_loss, mean_loss + std_loss, \n",
    "                         alpha=0.2, color=colors[idx])\n",
    "    \n",
    "    ax.set_xlabel('Training Steps', fontsize=14)\n",
    "    ax.set_ylabel('Test Loss (Cross-Entropy)', fontsize=14)\n",
    "    ax.set_title(f'Vocabulary Size = {vocab_size}: Comparison Across Beta Values', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=12, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Compare Vocabulary Sizes for Each Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta in beta_values:\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    colors = plt.cm.plasma(np.linspace(0, 1, len(vocab_sizes)))\n",
    "    \n",
    "    for idx, vocab_size in enumerate(vocab_sizes):\n",
    "        if len(all_dynamics[vocab_size][beta]) == 0:\n",
    "            continue\n",
    "        \n",
    "        steps = all_dynamics[vocab_size][beta][0]['steps']\n",
    "        \n",
    "        # Collect all losses\n",
    "        all_losses = []\n",
    "        for dynamics in all_dynamics[vocab_size][beta]:\n",
    "            if len(dynamics['steps']) == len(steps):\n",
    "                all_losses.append(dynamics['test_loss'])\n",
    "        \n",
    "        if len(all_losses) == 0:\n",
    "            continue\n",
    "        \n",
    "        all_losses = np.array(all_losses)\n",
    "        mean_loss = np.mean(all_losses, axis=0)\n",
    "        std_loss = np.std(all_losses, axis=0)\n",
    "        \n",
    "        # Plot mean\n",
    "        ax.plot(steps, mean_loss, linewidth=2.5, label=f'V={vocab_size}', color=colors[idx])\n",
    "        ax.fill_between(steps, mean_loss - std_loss, mean_loss + std_loss, \n",
    "                         alpha=0.2, color=colors[idx])\n",
    "    \n",
    "    ax.set_xlabel('Training Steps', fontsize=14)\n",
    "    ax.set_ylabel('Test Loss (Cross-Entropy)', fontsize=14)\n",
    "    ax.set_title(f'Beta = {beta}: Comparison Across Vocabulary Sizes', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=12, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: Final Loss vs Theoretical Minimum (Heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices for heatmap\n",
    "final_loss_matrix = np.zeros((len(vocab_sizes), len(beta_values)))\n",
    "theoretical_matrix = np.zeros((len(vocab_sizes), len(beta_values)))\n",
    "gap_matrix = np.zeros((len(vocab_sizes), len(beta_values)))\n",
    "\n",
    "for i, vocab_size in enumerate(vocab_sizes):\n",
    "    for j, beta in enumerate(beta_values):\n",
    "        if len(all_dynamics[vocab_size][beta]) > 0:\n",
    "            # Final losses\n",
    "            final_losses = [dynamics['test_loss'][-1] for dynamics in all_dynamics[vocab_size][beta]]\n",
    "            final_loss_matrix[i, j] = np.mean(final_losses)\n",
    "            \n",
    "            # Theoretical minimum\n",
    "            if len(theoretical_minimums[vocab_size][beta]) > 0:\n",
    "                theoretical_matrix[i, j] = np.mean(theoretical_minimums[vocab_size][beta])\n",
    "                gap_matrix[i, j] = final_loss_matrix[i, j] - theoretical_matrix[i, j]\n",
    "        else:\n",
    "            final_loss_matrix[i, j] = np.nan\n",
    "            theoretical_matrix[i, j] = np.nan\n",
    "            gap_matrix[i, j] = np.nan\n",
    "\n",
    "# Plot heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Final loss\n",
    "im1 = axes[0].imshow(final_loss_matrix, cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_xticks(range(len(beta_values)))\n",
    "axes[0].set_xticklabels([f'{b}' for b in beta_values])\n",
    "axes[0].set_yticks(range(len(vocab_sizes)))\n",
    "axes[0].set_yticklabels([f'{v}' for v in vocab_sizes])\n",
    "axes[0].set_xlabel('Beta', fontsize=12)\n",
    "axes[0].set_ylabel('Vocabulary Size', fontsize=12)\n",
    "axes[0].set_title('Final Test Loss', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Theoretical minimum\n",
    "im2 = axes[1].imshow(theoretical_matrix, cmap='YlOrRd', aspect='auto')\n",
    "axes[1].set_xticks(range(len(beta_values)))\n",
    "axes[1].set_xticklabels([f'{b}' for b in beta_values])\n",
    "axes[1].set_yticks(range(len(vocab_sizes)))\n",
    "axes[1].set_yticklabels([f'{v}' for v in vocab_sizes])\n",
    "axes[1].set_xlabel('Beta', fontsize=12)\n",
    "axes[1].set_ylabel('Vocabulary Size', fontsize=12)\n",
    "axes[1].set_title('Theoretical Minimum', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Gap\n",
    "im3 = axes[2].imshow(gap_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[2].set_xticks(range(len(beta_values)))\n",
    "axes[2].set_xticklabels([f'{b}' for b in beta_values])\n",
    "axes[2].set_yticks(range(len(vocab_sizes)))\n",
    "axes[2].set_yticklabels([f'{v}' for v in vocab_sizes])\n",
    "axes[2].set_xlabel('Beta', fontsize=12)\n",
    "axes[2].set_ylabel('Vocabulary Size', fontsize=12)\n",
    "axes[2].set_title('Optimization Gap', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY: Vocabulary & Beta Experiments\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{'Beta':<8} {'Theoretical':<15} {'Final Loss':<15} {'Gap':<12} {'% Above':<10} {'Final Acc':<12}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for beta in beta_values:\n",
    "        if len(all_dynamics[vocab_size][beta]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Theoretical minimum\n",
    "        theo_mean = np.mean(theoretical_minimums[vocab_size][beta])\n",
    "        theo_std = np.std(theoretical_minimums[vocab_size][beta])\n",
    "        \n",
    "        # Final losses\n",
    "        final_losses = [dynamics['test_loss'][-1] for dynamics in all_dynamics[vocab_size][beta]]\n",
    "        final_mean = np.mean(final_losses)\n",
    "        final_std = np.std(final_losses)\n",
    "        \n",
    "        # Gap\n",
    "        gap = final_mean - theo_mean\n",
    "        pct_above = (gap / theo_mean) * 100\n",
    "        \n",
    "        # Final accuracy\n",
    "        final_accs = [dynamics['test_acc'][-1] for dynamics in all_dynamics[vocab_size][beta]]\n",
    "        acc_mean = np.mean(final_accs)\n",
    "        acc_std = np.std(final_accs)\n",
    "        \n",
    "        print(f\"{beta:<8.1f} {theo_mean:>6.4f}±{theo_std:<5.4f} {final_mean:>6.4f}±{final_std:<5.4f} \"\n",
    "              f\"{gap:>6.4f}      {pct_above:>6.2f}%    {acc_mean:>5.4f}±{acc_std:<5.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
